{
  "name": "RETRIEVAL",
  "nodes": [
    {
      "parameters": {},
      "name": "Cron (Daily Warm-up)",
      "type": "n8n-nodes-base.cron",
      "typeVersion": 1,
      "position": [
        680,
        1680
      ],
      "id": "71c795f1-1614-4223-ab29-0d45c2371d80"
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "-- Select document summaries or metadata for warm-up\n-- Customize this query to retrieve the specific context you want to preload\nSELECT \n    d.document_title AS title,\n    string_agg(c.section_title || ': ' || LEFT(c.chunk_text, 500), '\n') AS summary\nFROM documents d\nJOIN cag_document_registry c ON d.document_id = c.document_id\nWHERE d.status = 'cached'\nGROUP BY d.document_id, d.document_title",
        "additionalFields": {}
      },
      "name": "Retrieve Context",
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 1,
      "position": [
        900,
        1680
      ],
      "id": "42fe713c-8189-48b9-9036-4f4dd99f029f"
    },
    {
      "parameters": {
        "functionCode": "const items = $input.all();\nconst warmUpPrompts = [];\n\nfor (const item of items) {\n  const { title, summary } = item.json;\n  // Construct a structured warm-up prompt\n  const prompt = `[WARM-UP] Document: ${title}\\nContext Summary:\\n${summary}\\n[END WARM-UP]`;\n  warmUpPrompts.push({ json: { prompt } });\n}\n\nreturn warmUpPrompts;"
      },
      "name": "Prepare Warm-Up Prompts",
      "type": "n8n-nodes-base.function",
      "typeVersion": 1,
      "position": [
        1120,
        1680
      ],
      "id": "0960f386-f42c-4736-a6e1-80671f52eecb"
    },
    {
      "parameters": {
        "batchSize": 1,
        "options": {}
      },
      "name": "Split In Batches",
      "type": "n8n-nodes-base.splitInBatches",
      "typeVersion": 2,
      "position": [
        1340,
        1680
      ],
      "id": "5f51b294-4c99-4b17-9910-081a8591c323"
    },
    {
      "parameters": {
        "url": "={{ $env.LLAMACPP_SERVER_URL }}/completion",
        "sendBody": true,
        "bodyParameters": {
          "parameters": [
            {
              "name": "prompt",
              "value": "={{ $json.prompt }}"
            },
            {
              "name": "cache_prompt",
              "value": true
            },
            {
              "name": "n_predict",
              "value": 0
            },
            {
              "name": "n_keep",
              "value": "={{ $json.prompt.length + 1 }}"
            }
          ]
        },
        "options": {}
      },
      "name": "Warm-Up Inference",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 3,
      "position": [
        1560,
        1680
      ],
      "id": "15d720c2-bf52-48bf-9da2-174ad5ed7658"
    },
    {
      "parameters": {
        "functionCode": "// Simple logging of warm-up status\nconst { prompt } = $json;\nconsole.log(`Warm-up prompt sent: ${prompt.substring(0, 50)}...`);\nreturn $input.all();"
      },
      "name": "Log Warm-Up",
      "type": "n8n-nodes-base.function",
      "typeVersion": 1,
      "position": [
        1780,
        1680
      ],
      "id": "9cb3696f-4060-4ed9-8b3c-edf1a717f3c2"
    },
    {
      "parameters": {
        "httpMethod": "POST",
        "path": "query",
        "responseMode": "responseNode",
        "options": {}
      },
      "name": "Webhook",
      "type": "n8n-nodes-base.webhook",
      "typeVersion": 1,
      "position": [
        660,
        2220
      ],
      "webhookId": "f1a6c1f0-d1e8-4f07-b824-8c72f1c1e1d0",
      "id": "070ed37c-b668-4f41-ac2b-e90dfaba453e"
    },
    {
      "parameters": {
        "functionCode": "const items = $input.all();\nlet context = \"\";\n\nif (!items || items.length === 0 || !items[0].json) {\n  return [{ json: { context: \"[No relevant context found]\", foundContext: false } }];\n}\n\nfor (const item of items) {\n  const { section_title, chunk_text, document_title, document_id } = item.json;\n  context += `Document: ${document_title} (ID: ${document_id})\\n`;\n  context += `Section: ${section_title}\\n`;\n  context += `Chunk: ${chunk_text}\\n\\n`;\n}\n\nreturn [{ json: { context: context.trim(), foundContext: true } }];"
      },
      "name": "Aggregate Context",
      "type": "n8n-nodes-base.function",
      "typeVersion": 1,
      "position": [
        1120,
        2220
      ],
      "id": "c32d8926-144f-48b4-bfe2-a5928fa64b31"
    },
    {
      "parameters": {
        "functionCode": "const userQuery = $input.item.json.query || $workflow.execution.data.initialInput.body.query;\nconst context = $input.item.json.context;\n\n// Build the final composite prompt\nconst prompt = `Context:\\n${context}\\n\\nUser Query: ${userQuery}\\n\\nAnswer:`;\n\nreturn [{ json: { prompt, userQuery, context } }];"
      },
      "name": "Construct Prompt",
      "type": "n8n-nodes-base.function",
      "typeVersion": 1,
      "position": [
        1340,
        2220
      ],
      "id": "3e455dd9-3ac1-4fc2-8438-490a7417e4cf"
    },
    {
      "parameters": {
        "url": "={{ $env.LLAMACPP_SERVER_URL }}/completion",
        "sendBody": true,
        "bodyParameters": {
          "parameters": [
            {
              "name": "prompt",
              "value": "={{ $json.prompt }}"
            },
            {
              "name": "n_predict",
              "value": 512
            }
          ]
        },
        "options": {}
      },
      "name": "Inference",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 3,
      "position": [
        1560,
        2220
      ],
      "id": "66edb332-f418-4204-9eec-3f03dfa87548"
    },
    {
      "parameters": {
        "functionCode": "const llmResponse = $input.item.json.content;\nconst userQuery = $input.item.json.userQuery;\n\n// Process and trim the LLM response\nconst answer = llmResponse.trim();\n\nreturn [{ json: { answer, query: userQuery, context: $input.item.json.context } }];"
      },
      "name": "Process Response",
      "type": "n8n-nodes-base.function",
      "typeVersion": 1,
      "position": [
        1780,
        2220
      ],
      "id": "d7ae5fb8-e352-4462-b46c-240cc04bf040"
    },
    {
      "parameters": {
        "options": {}
      },
      "name": "Respond to Webhook",
      "type": "n8n-nodes-base.respondToWebhook",
      "typeVersion": 1,
      "position": [
        2220,
        2220
      ],
      "id": "eeeed146-ae27-49d9-84f4-8b6451c2eb47"
    },
    {
      "parameters": {
        "functionCode": "const processingTime = Date.now() - $workflow.execution.startedAt;\nconst contextLength = $input.item.json.context ? $input.item.json.context.length : 0;\n\nreturn [{ json: { processingTime, contextLength, query: $input.item.json.query } }];"
      },
      "name": "Log Metrics",
      "type": "n8n-nodes-base.function",
      "typeVersion": 1,
      "position": [
        2000,
        2220
      ],
      "id": "29bc0104-7240-40c7-b5b4-880829e1dffb"
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "-- Retrieve context chunks based on semantic similarity\nWITH user_query AS (\n  SELECT '{{ $json.body.query }}'::text AS query_text\n)\nSELECT\n  c.chunk_id,\n  c.section_title,\n  c.chunk_text,\n  d.document_title,\n  d.document_id\nFROM cag_document_registry c\nJOIN documents d ON c.document_id = d.document_id\nWHERE d.status = 'cached'\n  AND c.cag_status = 'cached'\n  AND c.chunk_text ILIKE '%' || (SELECT query_text FROM user_query) || '%'\nORDER BY c.chunk_text ILIKE '%' || (SELECT query_text FROM user_query) || '%'\nLIMIT 5;",
        "additionalFields": {}
      },
      "name": "Retrieve Context1",
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 1,
      "position": [
        900,
        2220
      ],
      "id": "8e647c74-677d-464b-8ccf-12e96cf1b8ca"
    },
    {
      "parameters": {
        "url": "={{ $env.KNOWLEDGE_GRAPH_API_URL }}/query",
        "sendBody": true,
        "bodyParameters": {
          "parameters": [
            {
              "name": "query",
              "value": "={{ $json.body.query }}"
            }
          ]
        },
        "options": {}
      },
      "name": "Query Knowledge Graph",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 3,
      "position": [
        900,
        2700
      ],
      "id": "31744b76-4b62-428b-8110-72ab47cba8af"
    },
    {
      "parameters": {
        "functionCode": "// Format the facts retrieved from the knowledge graph\n// Assuming the knowledge graph returns an array of facts\nconst items = $input.all();\n\nif (!items || items.length === 0 || !items[0].json) { \n  return [{ json: { facts: '[No relevant facts found]', hasFacts: false } }];\n}\n\nlet facts = \"\";\nfor (const item of items) {\n  // Adapt this to the actual structure of your knowledge graph response\n  if (item.json.fact) {\n    facts += `- ${item.json.fact}\\n`;\n  } else if (item.json.subject && item.json.predicate && item.json.object) {\n    facts += `- ${item.json.subject} ${item.json.predicate} ${item.json.object}\\n`;\n  }\n}\n\nreturn [{ json: { facts: facts.trim(), hasFacts: true } }];"
      },
      "name": "Format Facts",
      "type": "n8n-nodes-base.function",
      "typeVersion": 1,
      "position": [
        1120,
        2700
      ],
      "id": "2a1745a5-5d76-4ccb-a65d-fd4be4a12f4e"
    },
    {
      "parameters": {
        "functionCode": "// Combine dynamic context, knowledge graph facts, and the user query\nconst userQuery = $input.item.json.query || $workflow.execution.data.initialInput.body.query;\nconst facts = $input.item.json.facts || \"[No relevant facts found]\";\n\nlet prompt = \"\";\n\n// Add facts if available\nif ($input.item.json.hasFacts) {\n  prompt += `Facts:\\n${facts}\\n\\n`;\n}\n\n// Append the user query\nprompt += `User Query: ${userQuery}\\n\\nAnswer:`;\n\nreturn [{ json: { prompt, userQuery, facts } }];"
      },
      "name": "Combine Context and Facts",
      "type": "n8n-nodes-base.function",
      "typeVersion": 1,
      "position": [
        1340,
        2700
      ],
      "id": "c2fbba7b-4310-4936-89a8-56b291060689"
    },
    {
      "parameters": {
        "httpMethod": "POST",
        "path": "query_kag",
        "responseMode": "responseNode",
        "options": {}
      },
      "name": "Webhook1",
      "type": "n8n-nodes-base.webhook",
      "typeVersion": 1,
      "position": [
        680,
        2700
      ],
      "webhookId": "a8b3f920-7c4d-4e6a-9b15-6f87e9d2a0c3",
      "id": "cb206a08-693c-414b-9b9b-c23587cf6b6d"
    },
    {
      "parameters": {
        "url": "={{ $env.LLAMACPP_SERVER_URL }}/completion",
        "sendBody": true,
        "bodyParameters": {
          "parameters": [
            {
              "name": "prompt",
              "value": "={{ $json.prompt }}"
            },
            {
              "name": "n_predict",
              "value": 512
            }
          ]
        },
        "options": {}
      },
      "name": "Inference1",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 3,
      "position": [
        1560,
        2700
      ],
      "id": "1e42670a-67e3-419f-b7f3-924a4a3f200c"
    },
    {
      "parameters": {
        "functionCode": "const llmResponse = $input.item.json.content;\nconst userQuery = $input.item.json.userQuery;\nconst facts = $input.item.json.facts;\n\nconst answer = llmResponse.trim();\n\nreturn [{ json: { answer, query: userQuery, facts } }];"
      },
      "name": "Process Response1",
      "type": "n8n-nodes-base.function",
      "typeVersion": 1,
      "position": [
        1780,
        2700
      ],
      "id": "5c3b1b8d-0dd9-4423-b2c3-fc7d315c2206"
    },
    {
      "parameters": {
        "options": {}
      },
      "name": "Respond to Webhook1",
      "type": "n8n-nodes-base.respondToWebhook",
      "typeVersion": 1,
      "position": [
        2220,
        2700
      ],
      "id": "ce761d69-7631-4440-9f11-064b1850bc05"
    },
    {
      "parameters": {
        "functionCode": "// Log performance and usage data\nconst processingTime = Date.now() - $workflow.execution.startedAt;\n\nreturn [{ json: { processingTime, query: $input.item.json.query, factsUsed: $input.item.json.hasFacts } }];"
      },
      "name": "Log Metrics1",
      "type": "n8n-nodes-base.function",
      "typeVersion": 1,
      "position": [
        2000,
        2700
      ],
      "id": "9cacd1ce-62bf-472a-a4c1-fa8e535a6fa1"
    },
    {
      "parameters": {
        "content": "![Screenshot of workflow](https://raw.githubusercontent.com/AbelCoplet/llama-cag-n8N/main/docs/images/Screenshot%202025-03-20%20at%2020.02.09.png)",
        "height": 800,
        "width": 1020
      },
      "type": "n8n-nodes-base.stickyNote",
      "typeVersion": 1,
      "position": [
        1120,
        0
      ],
      "id": "d283eebe-f313-4c65-9622-1cfd9694bfeb",
      "name": "Sticky Note2"
    },
    {
      "parameters": {
        "content": "![Screenshot 1](https://raw.githubusercontent.com/AbelCoplet/llama-cag-n8N/main/docs/images/Capture%20d'%C3%A9cran%202025-03-21%20004453.png)",
        "height": 800,
        "width": 1120
      },
      "type": "n8n-nodes-base.stickyNote",
      "typeVersion": 1,
      "position": [
        0,
        0
      ],
      "id": "9c35f856-f612-402b-b135-dece44704eb8",
      "name": "Sticky Note"
    },
    {
      "parameters": {
        "content": "![Screenshot 2](https://raw.githubusercontent.com/AbelCoplet/llama-cag-n8N/main/docs/images/Capture%20d'%C3%A9cran%202025-03-21%20004523.png)",
        "height": 460,
        "width": 640
      },
      "type": "n8n-nodes-base.stickyNote",
      "typeVersion": 1,
      "position": [
        0,
        1540
      ],
      "id": "a074b0a3-e1bc-47a3-a0ce-37c6ac0d3c54",
      "name": "Sticky Note1"
    },
    {
      "parameters": {
        "content": "![Screenshot 3](https://raw.githubusercontent.com/AbelCoplet/llama-cag-n8N/main/docs/images/Capture%20d'%C3%A9cran%202025-03-21%20004536.png)",
        "height": 460,
        "width": 640
      },
      "type": "n8n-nodes-base.stickyNote",
      "typeVersion": 1,
      "position": [
        0,
        2020
      ],
      "id": "37b9518f-c44d-4969-9716-30c25bc94eb5",
      "name": "Sticky Note3"
    },
    {
      "parameters": {
        "content": "![Screenshot 4](https://raw.githubusercontent.com/AbelCoplet/llama-cag-n8N/main/docs/images/Capture%20d'%C3%A9cran%202025-03-21%20004547.png)",
        "height": 460,
        "width": 640
      },
      "type": "n8n-nodes-base.stickyNote",
      "typeVersion": 1,
      "position": [
        0,
        2520
      ],
      "id": "b15256b2-6172-46b7-9671-24e610b3729e",
      "name": "Sticky Note4"
    },
    {
      "parameters": {
        "content": "![Screenshot 2025-03-20 at 20.02.49](https://raw.githubusercontent.com/AbelCoplet/llama-cag-n8N/main/docs/images/Screenshot%202025-03-20%20at%2020.02.49.png)",
        "height": 460,
        "width": 640
      },
      "type": "n8n-nodes-base.stickyNote",
      "typeVersion": 1,
      "position": [
        0,
        800
      ],
      "id": "f16945e0-6932-45ca-8657-d36798899e6e",
      "name": "Sticky Note5"
    },
    {
      "parameters": {
        "content": "## CAG Retrieval from Local LLM . KV Cache management script separate\n**Double click** to edit me. [Guide](https://docs.n8n.io/workflows/sticky-notes/)",
        "height": 460,
        "width": 1800,
        "color": 2
      },
      "type": "n8n-nodes-base.stickyNote",
      "typeVersion": 1,
      "position": [
        640,
        800
      ],
      "id": "5c9f02b9-8080-40c9-9006-b65d167dea5f",
      "name": "Sticky Note6"
    },
    {
      "parameters": {
        "content": "## CAG Simulation \"Warm Up\"",
        "height": 460,
        "width": 1800,
        "color": 3
      },
      "type": "n8n-nodes-base.stickyNote",
      "typeVersion": 1,
      "position": [
        640,
        1540
      ],
      "id": "ceb5d966-aad9-452e-8fde-0f3da4f5f24d",
      "name": "Sticky Note7"
    },
    {
      "parameters": {
        "content": "## Feeding Whole Prompt at Querry\n**Double click** to edit me. [Guide](https://docs.n8n.io/workflows/sticky-notes/)",
        "height": 460,
        "width": 1800,
        "color": 4
      },
      "type": "n8n-nodes-base.stickyNote",
      "typeVersion": 1,
      "position": [
        640,
        2020
      ],
      "id": "f0247630-3069-4ac6-b461-89119bded693",
      "name": "Sticky Note8"
    },
    {
      "parameters": {
        "content": "## KAG\n**Double click** to edit me. [Guide](https://docs.n8n.io/workflows/sticky-notes/)",
        "height": 460,
        "width": 1820,
        "color": 5
      },
      "type": "n8n-nodes-base.stickyNote",
      "typeVersion": 1,
      "position": [
        640,
        2520
      ],
      "id": "43b1c13c-6a06-4fbc-9e7b-c4399895952b",
      "name": "Sticky Note9"
    },
    {
      "parameters": {
        "url": "http://localhost:8000/query",
        "sendHeaders": true,
        "headerParameters": {
          "parameters": [
            {
              "name": "Content-Type",
              "value": "application/json"
            }
          ]
        },
        "sendBody": true,
        "bodyParameters": {
          "parameters": [
            {
              "name": "query",
              "value": "={{ $json.query }}"
            },
            {
              "name": "maxTokens",
              "value": "={{ $json.maxTokens || 1024 }}"
            },
            {
              "name": "temperature",
              "value": "={{ $json.temperature || 0.7 }}"
            }
          ]
        },
        "options": {}
      },
      "name": "Query CAG Model",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.1,
      "position": [
        880,
        1000
      ],
      "id": "091549f0-3e05-4d7b-973e-5e9925667c20"
    },
    {
      "parameters": {
        "jsCode": "// Format response for user\nconst responseData = $input.item.json.data || {};\n\nreturn $input.item.json = {\n  query: $input.item.json.query || responseData.query,\n  response: responseData.response || 'No response received',\n  success: responseData.success !== false,\n  error: responseData.error || null,\n  processedAt: new Date().toISOString()\n};"
      },
      "name": "Format Response",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1080,
        1000
      ],
      "id": "31569a77-4962-4895-bf78-f4d9760db21a"
    },
    {
      "parameters": {
        "respondWith": "json",
        "responseBody": "={{ $input.item.json }}",
        "options": {}
      },
      "name": "Return Response",
      "type": "n8n-nodes-base.respondToWebhook",
      "typeVersion": 1,
      "position": [
        1280,
        1000
      ],
      "id": "6c1408ab-4336-4336-8a5d-f5847a0898e0"
    },
    {
      "parameters": {
        "workflowInputs": {
          "values": [
            {}
          ]
        }
      },
      "type": "n8n-nodes-base.executeWorkflowTrigger",
      "typeVersion": 1.1,
      "position": [
        680,
        1000
      ],
      "id": "e28fa3fa-b358-4ff8-ba3e-7e99e5149acb",
      "name": "When Executed by Another Workflow"
    },
    {
      "parameters": {
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.chatTrigger",
      "typeVersion": 1.1,
      "position": [
        1720,
        860
      ],
      "id": "645cd76f-8d84-4aea-9284-907caf028bb9",
      "name": "When chat message received",
      "webhookId": "3446d06b-d139-42ed-9a32-e8db9928b015"
    },
    {
      "parameters": {
        "model": {
          "__rl": true,
          "mode": "list",
          "value": "claude-3-7-sonnet-20250219",
          "cachedResultName": "Claude 3.7 Sonnet"
        },
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.lmChatAnthropic",
      "typeVersion": 1.3,
      "position": [
        1900,
        1100
      ],
      "id": "8e1e65a3-32f1-4f81-bea1-0e3a39208a0f",
      "name": "Anthropic Chat Model"
    },
    {
      "parameters": {},
      "type": "@n8n/n8n-nodes-langchain.toolWorkflow",
      "typeVersion": 2.1,
      "position": [
        2200,
        1100
      ],
      "id": "6db86ff7-d2e2-437d-b053-0dcbfb5e99de",
      "name": "Call n8n Workflow Tool"
    },
    {
      "parameters": {
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.agent",
      "typeVersion": 1.8,
      "position": [
        2020,
        860
      ],
      "id": "56699368-9545-4ea7-a91e-c2e63144264d",
      "name": "PLACEHOLDER AGENT"
    },
    {
      "parameters": {
        "content": "## Configuration Instructions for Retrieval Workflow\n1. Environment Variable Configuration\nThe workflow references $env.LLAMACPP_SERVER_URL which needs to be set in n8n:\n\nIn n8n, go to Settings (gear icon) → Variables\nAdd a new variable:\n\nName: LLAMACPP_SERVER_URL\nValue: http://cag-bridge:8000 (if using Docker setup) or http://localhost:8000 (if running locally)\n\n\nSave your changes\n\n2. HTTP Request Node Configuration\nThe \"Query CAG Model\" node needs to be configured correctly:\n\nSelect the \"Query CAG Model\" node\nVerify the URL is set to {{ $env.LLAMACPP_SERVER_URL }}/query (or update it if needed)\nEnsure the Content-Type header is set to application/json\nVerify the body parameters contain:\n\nquery: ={{ $json.query }}\nmaxTokens: ={{ $json.maxTokens || 1024 }}\ntemperature: ={{ $json.temperature || 0.7 }}\n\n\n\n3. Master KV Cache Creation\nBefore using the retrieval workflow, ensure a master KV cache exists:\n\nRun the KV Cache Management workflow with the setAsMaster option set to true\nCheck that the master KV cache was created at the configured path (default: /data/kv_caches/master_cache.bin)\nNote: The CAG Bridge assumes this master cache exists for all queries\n\n4. Testing the Workflow\n\nCreate a test execution with input parameters:\njsonCopier{\n  \"query\": \"What are the key features of this system?\",\n  \"maxTokens\": 1024,\n  \"temperature\": 0.7\n}\n\nExecute the workflow and verify the response is properly formatted\n\n5. Endpoint Usage\nWhen integrating with other applications, use:\nCopierPOST http://localhost:5678/webhook/cag/query\nContent-Type: application/json\n\n{\n  \"query\": \"Your question about the document content\",\n  \"maxTokens\": 1024,\n  \"temperature\": 0.7\n}\nAdditional Recommendations\n\nError Handling: Consider adding error handling nodes to handle cases where the CAG Bridge is unavailable or the KV cache doesn't exist\nLogging: Add a node to log queries and responses for monitoring and debugging\nCaching: Consider implementing response caching for frequent queries to reduce load on the model",
        "height": 1260,
        "width": 720
      },
      "type": "n8n-nodes-base.stickyNote",
      "typeVersion": 1,
      "position": [
        -720,
        0
      ],
      "id": "90e6f864-ae6f-4b50-ae6f-a78c8057ef96",
      "name": "Sticky Note10"
    },
    {
      "parameters": {
        "content": "## Work in Progess\n\nAdditional Methods for Managing \"Memory\"",
        "height": 260,
        "width": 2440,
        "color": 6
      },
      "type": "n8n-nodes-base.stickyNote",
      "typeVersion": 1,
      "position": [
        0,
        1280
      ],
      "id": "405aa1d5-96d1-453e-bf27-9d5a421691a1",
      "name": "Sticky Note11"
    },
    {
      "parameters": {
        "content": "## Data Memory Management for Large Language Models: Project Overview\nWhat This Project Is About\nThis project develops a sophisticated system for enhancing how large language models (LLMs) work with documents using a technique called Context-Augmented Generation (CAG). Unlike traditional approaches that slice documents into tiny chunks, this system leverages large context window models (128K tokens) to process entire documents and store their \"memory state\" for quick recall later.\nThink of it like this: instead of giving an LLM random snippets of a book each time it's asked a question, this system lets the model \"read the whole book once\" and remember everything, making future questions much faster and more accurate.\nMain Scenarios & Concepts\n1. Context-Augmented Generation (KV Cache Management)\nWhat it is: The core innovation of this project - using \"memory snapshots\" (KV caches) of documents that models can instantly access.\nHow it works:\n\nDocuments are processed once by large context models\nThe model's internal state (its \"thoughts\" about the document) is saved as a KV cache\nFuture queries use this pre-computed knowledge without re-reading the document\n\nKey components: Document processing workflow, CAG Bridge, KV cache storage\n2. Document Optimization\nWhat it is: An intelligent preprocessing step that prepares documents for efficient storage.\nHow it works:\n\nUses Claude to clean up documents - removing redundancies and repetition\nPreserves the meaning and structure while optimizing token usage\nMakes documents more efficient for storage in the model's context window\n\nKey components: Document optimization workflow, Claude API integration\n3. Model Warm-up\nWhat it is: A technique to \"prime\" the model with relevant knowledge before queries.\nHow it works:\n\nPeriodically feeds document summaries to the model\nKeeps important information in the model's \"working memory\"\nImproves response speed and quality for frequent topics\n\nKey components: Scheduled triggers, document summary generation\n4. Knowledge-Augmented Generation (KAG)\nWhat it is: An extension that combines document knowledge with structured facts.\nHow it works:\n\nQueries structured knowledge sources alongside documents\nBlends factual information with contextual understanding\nProvides more comprehensive and accurate responses\n\nKey components: Knowledge graph integration, fact formatting\n5. System Maintenance\nWhat it is: Automated housekeeping to keep the system running efficiently.\nHow it works:\n\nCleans up temporary files and unused caches\nMonitors system health and usage patterns\nMaintains performance as document collections grow\n\nKey components: Maintenance workflow, database monitoring\n6. Retrieval-Augmented Generation (RAG)\nWhat it is: The popular approach of retrieving relevant chunks from a vector database to augment LLM responses.\nHow it fits in this design:\nWhile CAG is the primary focus of this project, RAG is intentionally designed as a complementary approach that can be called by the intelligent agent. The design intent allows for flexibility in how questions are answered:\nHow it works in this system:\n\nThe agent can evaluate each user query and decide the best approach\nFor some questions, the agent might call the CAG system for deep document knowledge\nFor others, it might call a RAG system for broader but shallower knowledge\nThe agent can even combine outputs from both systems for comprehensive answers\n\nWhy not the main focus: The project doesn't emphasize RAG implementation because there are already many excellent templates and implementations available. Instead, it focuses on the more innovative CAG approach while ensuring RAG compatibility.",
        "height": 1700,
        "width": 720,
        "color": 6
      },
      "type": "n8n-nodes-base.stickyNote",
      "typeVersion": 1,
      "position": [
        -720,
        1280
      ],
      "id": "62bc15e6-8ad2-47b5-b651-1b8d54189445",
      "name": "Sticky Note12"
    }
  ],
  "pinData": {},
  "connections": {
    "Cron (Daily Warm-up)": {
      "main": [
        [
          {
            "node": "Retrieve Context",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Retrieve Context": {
      "main": [
        [
          {
            "node": "Prepare Warm-Up Prompts",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Prepare Warm-Up Prompts": {
      "main": [
        [
          {
            "node": "Split In Batches",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Split In Batches": {
      "main": [
        [
          {
            "node": "Warm-Up Inference",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Warm-Up Inference": {
      "main": [
        [
          {
            "node": "Log Warm-Up",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Webhook": {
      "main": [
        [
          {
            "node": "Retrieve Context1",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Aggregate Context": {
      "main": [
        [
          {
            "node": "Construct Prompt",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Construct Prompt": {
      "main": [
        [
          {
            "node": "Inference",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Inference": {
      "main": [
        [
          {
            "node": "Process Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Process Response": {
      "main": [
        [
          {
            "node": "Log Metrics",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Log Metrics": {
      "main": [
        [
          {
            "node": "Respond to Webhook",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Retrieve Context1": {
      "main": [
        [
          {
            "node": "Aggregate Context",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Query Knowledge Graph": {
      "main": [
        [
          {
            "node": "Format Facts",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Format Facts": {
      "main": [
        [
          {
            "node": "Combine Context and Facts",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Combine Context and Facts": {
      "main": [
        [
          {
            "node": "Inference1",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Webhook1": {
      "main": [
        [
          {
            "node": "Query Knowledge Graph",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Inference1": {
      "main": [
        [
          {
            "node": "Process Response1",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Process Response1": {
      "main": [
        [
          {
            "node": "Log Metrics1",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Log Metrics1": {
      "main": [
        [
          {
            "node": "Respond to Webhook1",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Query CAG Model": {
      "main": [
        [
          {
            "node": "Format Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Format Response": {
      "main": [
        [
          {
            "node": "Return Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "When Executed by Another Workflow": {
      "main": [
        [
          {
            "node": "Query CAG Model",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "When chat message received": {
      "main": [
        [
          {
            "node": "PLACEHOLDER AGENT",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Anthropic Chat Model": {
      "ai_languageModel": [
        [
          {
            "node": "PLACEHOLDER AGENT",
            "type": "ai_languageModel",
            "index": 0
          }
        ]
      ]
    },
    "Call n8n Workflow Tool": {
      "ai_tool": [
        [
          {
            "node": "PLACEHOLDER AGENT",
            "type": "ai_tool",
            "index": 0
          }
        ]
      ]
    }
  },
  "active": false,
  "settings": {
    "executionOrder": "v1"
  },
  "versionId": "96ef11ef-8c0c-4cb7-8c72-873c0f3012fa",
  "meta": {
    "instanceId": "031b645215ae9b6f375e501b38c32a541dca72c391120eef6f1ad1015f3ee67f"
  },
  "id": "cLCMEjxOljNKVeXf",
  "tags": []
}